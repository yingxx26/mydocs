/usr/local/src/spark/bin/pyspark --master spark://storage9:7077 --driver-class-path /usr/local/src/spark/lib/mysql-connector-java-5.1.35-bin.jar

hdfs://storage9:9000/tmp/movies.

/usr/local/src/hadoop/bin/hdfs dfs -put  /usr/local/src/pylearn/datas/users.parquet

 val cityAccessTopNDF = accessDF.filter($"day" === day && $"cmsType" === "productId")
    .groupBy("day","cmsId").agg(sum("traffic").as("traffics"))
    .orderBy($"traffics".desc)

for data in result:
  print(data[0],":")
  for i in list(data[1]):
      print(i)



for data in result:
  print(data[0],":")
  for i in list(data[1]):
    for j in i:
      print(j)


sq = SQLContext(sc)
l=[('Tom',14),('Jerry',20)]
sq.createDataFrame(l).collect()




from pyspark import SparkConf
from pyspark.sql import SQLContext

conf = SparkConf()
conf.set('master','spark://storage9:7077')
sparkContext = SparkContext(conf=conf)
sparkContext.setLogLevel('WARN')

sqlContext = SQLContext(sparkContext)
l=[('Tom',14),('Jerry',20)]
print(sqlContext.createDataFrame(l).collect())
sparkContext.stop()


import numpy as np 
from pyspark.mllib.stat import Statistics 
mat = sc.parallelize( 
    [np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([3.0, 30.0, 300.0])],3 
)  
summary = Statistics.colStats(mat) 



from pyspark.ml.linalg import Vectors 
from pyspark.ml.classification import LogisticRegression 
sq = SQLContext(sc)
training = sq.createDataFrame([     (1.0, Vectors.dense([0.0, 1.1, 0.1])), 
    (0.0, Vectors.dense([2.0, 1.0, -1.0])), 
    (0.0, Vectors.dense([2.0, 1.3, 1.0])), 
    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], ["label", "features"]) 
