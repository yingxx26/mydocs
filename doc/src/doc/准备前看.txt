项目：

项目流程（数据采集flume（source channel sink），处理 spark，展示hbase javaweb） （还有巴巴运动网）

优化 静态化（添加修改产品时生成静态页面）， 二级缓存（@cache） ，权限 aop

遇到困难（数据倾斜，java 100%内存hashmap环）


知识点：

一次可执行task（并行度）（core数）  一次 要执行task（默认hdfs数据的block数，每个block128M）

direct读取kafka保证高可用，不需要再保存一份日志

新生代 复制， 老年代标记压缩
新生代到老年代 1：新生代surver满，触发minorgc 到老年代，装不下fullgc 2：长期存活的对象，3：大对象

spark启动原理    rdd（可以恢复，溢出在磁盘）  

数据倾斜，加机器，加并行度spark.default.parallelism SparkConf conf = new SparkConf().set("spark.default.parallelism", "500")
，persist，复用，提高reduce并行度，减少持久化比例，map输出端合并（设置hashshfulle ，spark.shuffle.consolidateFiles", "true")），

hashmap原理 concurrenthashmap1.7锁分段  1.8 红黑树    
如果没有初始化就先调用initTable（）方法来进行初始化过程
如果没有hash冲突就直接CAS插入
如果还在进行扩容操作就先进行扩容
如果存在hash冲突，就加锁来保证线程安全，这里有两种情况，一种是链表形式就直接遍历到尾端插入，一种是红黑树就按照红黑树结构插入，
最后一个如果该链表的数量大于阈值8，就要先转换成黑红树的结构，break再一次进入循环

tcp三次握手防止重复连接， 挥手要四次 终止信号和终止数据传输要分开发

https 对称加密的秘钥是非对称加密

netty 时间绑定channel注册到 selector  阻塞在selector   长连接 handler

堆算法 完全二叉树，左右子树高度不超过1

快排算法，找个随机值，左边比他大交换，右边比他小，交换。排好后，这个值得左右两边 执行同样操作

数据库中，通常默认隔离级别是“读已提交”，在默认的事务隔离级别下：insert，update，delete用的是排他锁, 会等待事务完成。通常情况下可以把隔离级别设为Read Committed，它能避免脏读，而且有较好的并发性能。尽管它会导致虚读、幻读等问题，在可能出现这类问题的个别场合可以由应用程序用悲观锁或乐观锁来控制。
排它锁又称为写锁 ，共享锁又称为读锁
数据库事务                         脏读（读到未提交），                         不可重复读（读到更新或者删除）                                                           ，  幻读（读到新增加）  
       读未提交（排他写锁，不加共享锁）       读已提交（瞬间共享读锁，排他写锁） ，  可重复读（共享读锁至事务结束，之前都是 行锁 ）写写阻塞，读旧版本，        串行化（用表锁解决，意向共享锁（IS） ）排他写锁，读写阻塞，悲观， 局部悲观锁（for update）
InnoDB行锁是通过给索引上的索引项加锁来实现的，因此InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！
cas   缓存lock前缀锁定,阻止同时修改缓存对应的内存区域   （缺点） aba，循环时间长,只能保证一个变量
 
violate 和cas对应，缓存一致性协议  ，别的缓存设置无效

Synchronized  monitor监视器锁

Aqs  线程 未获得锁加入 双向队列 尾部      cas violate 保证线程安全   

线程池（阻塞队列）

堆栈结构  内存模型 程序计数器，堆（对象，数组），栈（临时变量，引用），方法区（类信息），双亲委派机制（启动，扩展，系统），    最大内存和最小内存一样，调高内存（默认是4分之1）   调高新生代（默认1:2）

购物车session ，sessionid放cookie里实现 关闭浏览器购物车不丢失

1.接口是公开的，里面不能有私有的方法或变量，是用于让别人使用的，而抽象类是可以有私有方法或私有变量的，
2.接口不包含构造方法，抽象类里可以包含构造方法。    
3.另外，实现接口的一定要实现接口里定义的所有方法，而实现抽象类可以有选择地重写需要用到的方法，一般的应用里，最顶级的是接口，然后是抽象类实现接口，最后才到具体类实现。
4.还有，接口可以实现多重继承，而一个类只能继承一个超类，但可以通过继承多个接口实现多重继承，接口还有标识（里面没有任何方法，如Remote接口）和数据共享（里面的变量全是常量）的作用.
5.如果是抽象类要实现接口，可以实现部分或者一个都不实现就行，要是具体类就必须实现所有的方法

sql

工具：
           
springMVC   前端控制器拦截   转发到  处理器映射器， 映射到哪个handler（action，service，dao） ，在调用handler，  返回 modelandview ，渲染在页面上

Structs2  请求被FilterDispatcher接收，根据struts.xml配置，找到需要调用的Action类和方法，service，dao，返回 页面

机器学习  训练集，创建管道加入（分词，特征向量，算法）超网格参数设置， 评估器  ， 预测    
逻辑回归（sigmod函数） 决策树（商） 多层感知机（中间层sigmod，输出层softmax） 支持向量机（最近点到分割面最大距离） 线性回归（点到直线的距离差服从正太分布）
神经网络（概念）   正向传播求损失，反向传播调整 ， 激活函数（映射成非线性）
推荐系统 矩阵和低秩矩阵的差 正则化
 

parquet  压缩比高，分区过滤，列修剪

商品类别模块 前端递归 


 
 
在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。
 